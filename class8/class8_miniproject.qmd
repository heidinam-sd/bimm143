---
title: "Class 08: Unsupervised Learning Mini-project"
author: "Heidi Nam"
format: pdf
editor: visual
---

## Exploratory Data Analysis

We are using the excel file `"WisconsinCancer.csv"` as our data. We will download and read the csv file:

```{r}
fna.data <- "WisconsinCancer.csv"
wisc.df <- read.csv(fna.data, row.names=1)
```

As we don't need the diagnosis table within our data frame `wisc.df`, we will record it separately into a `diagnosis` vector.

```{r}
wisc.data <- wisc.df[,-1]
diagnosis <- factor(wisc.df[,1])
```

Answering questions 1\~3:

Q1: How many observations are in this dataset?

```{r}
nrow(wisc.df)
```

There are 569 observations in this dataset.

Q**2**. How many of the observations have a malignant diagnosis?

```{r}
sum(diagnosis == "M")
```

There are 212 observations with a malignant diagnosis.

**Q3**. How many variables/features in the data are suffixed with `_mean`?

```{r}
length(grep("_mean",colnames(wisc.data)))
```

There are 10 variables that are suffixed with `"_mean"`.

## Principal Component Analysis (PCA)

Checking if the data needs to be scaled before performing PCA:

```{r}
# Check column means and standard deviations
colMeans(wisc.data)

apply(wisc.data,2,sd)
```

Performing PCA with scaling:

```{r}
wisc.pr <- prcomp(wisc.data,scale. = TRUE)
summary(wisc.pr)
```

Answering questions 4-6:

**Q4**. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

A: 0.4427 (44.27%) of the original variance is captured by the first principal component PC1.

**Q5**. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

A: The first three principal components describe at least 70% of the original variance in the data.

**Q6**. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

A: The first seven principal components describe at least 90% of the original variance.

### Interpreting PCA results

Plotting the PCA results into a biplot:

```{r}
biplot(wisc.pr)
```

Q7: What stands out to you about this plot? Is it easy or difficult to understand? Why?

A: This plot is able to plot both the diagnosis number and the different variables and how they can be expressed on the plot with the first and second principal components. However, This is very difficult to understand as we don't understand why the plot originates from one center origin (and what it signifies) as well as have a lot of overlap of text and lines that makes the plot difficult to read.

Instead, plotting a scatter plot to observe the relation of the two principal components that are categorized by their diagnosis (malignant or benign).

```{r}
plot( wisc.pr$x[,1:2], col=diagnosis,
     xlab = "PC1", ylab = "PC2")
```

**Q8.** Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

```{r}
plot( wisc.pr$x[,c(1,3)], col=diagnosis,
     xlab = "PC1", ylab = "PC3")
```

The plot feels a lot more cluttered and its outliers feel a lot more variant as it has a higher range of PC3; thus, it feels like it doesn't optimally capture the data well.

Using ggplot to understand the data in a more visually aesthetic platform:

```{r}
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

library(ggplot2)

ggplot(df) + 
  aes(PC1, PC2, col=diagnosis) + 
  geom_point()
```

### Variance explained

Understanding the proportion of variance by calculating the variance of each principal component by taking is standard deviation and squaring it:

```{r}
# Calculate variance of each component
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

Calculating proportion of variance by dividing the variance by total variance of principal components:

```{r}
pve <- pr.var / sum(pr.var)
```

Plotting this into a scree plot:

```{r}
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

Presenting the same data in a bar plot:

```{r}
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

Plotting using a ggplot based graph:

```{r}
# install.packages("factoextra")
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```

**Q9.** For the first principal component, what is the component of the loading vector (i.e. `wisc.pr$rotation[,1]`) for the feature `concave.points_mean`? This tells us how much this original feature contributes to the first PC.

```{r}
loading_vector <- wisc.pr$rotation[,1]
loading_vector["concave.points_mean"]
```

The component of the loading vector is -0.2608538.

## Hierarchical Clustering

Scaling `wisc.data` by using the `scale()` function:

```{r}
data.scaled <- scale(wisc.data)
```

Calculating distances between all pairs of observations:

```{r}
data.dist <- dist(data.scaled)
```

Creating hierarchical clustering model using complete linkage:

```{r}
wisc.hclust <- hclust(data.dist, "complete")
```

Q10: Using the `plot()` and `abline()` functions, what is the height at which the clustering model has 4 clusters?

```{r}
plot(wisc.hclust)
abline(h=19, col="red", lty=2)
```

A: The height is 19 in where the clustering model has 4 clusters.

### Selecting number of clusters

Cutting the cluster tree to have 4 clusters:

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust,4)
```

Comparing cluster membership to the diagnoses:

```{r}
table(wisc.hclust.clusters, diagnosis)
```

### Using different methods

**Q12.** Which method gives your favorite results for the same `data.dist` dataset? Explain your reasoning.

A: I prefer the complete method when trying hierarchical clustering as it compares the max values of each "cluster" in order to judge similarity, and I believe that's a good way of comparison as it makes sure to contain all the data points into consideration.

## Combining methods

Creating hierarchical clustering model using method="ward.D2".

```{r}
wisc.pr.hclust <- hclust(data.dist, "ward.D2")
plot(wisc.pr.hclust)
```

Checking the clustering:

```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```

Checking the two cluster groups in terms of diagnosis:

```{r}
table(grps, diagnosis)
```

Plotting the principal components again categorized by the groups:

```{r}
plot(wisc.pr$x[,1:2], col=grps)
```

Comparing it to the graph where it is categorized by diagnosis:

```{r}
plot(wisc.pr$x[,1:2], col=diagnosis)
```

Changing the graph that was categorized by the groups to follow the same color scheme as the PCA diagnosis graph by changing order of 1 and 2 in grps:

```{r}
g <- as.factor(grps)
g <- relevel(g,2)
plot(wisc.pr$x[,1:2], col=g)
```

Using the distance along the first seven principal components for clustering so that we could compare:

```{r}
wisc.pr.hclust <- hclust(dist(wisc.pr$x[,1:7]), method="ward.D2")
```

cutting this hierarchical cluster into 2 clusters:

```{r}
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)
```

**Q13**. How well does the newly created model with four clusters separate out the two diagnoses?

```{r}
table(wisc.pr.hclust.clusters,diagnosis)
```

The newly created model helps separate the two diagnosis almost as well as the Euclidean distances taken from the data that was plotted with hierarchical clustering with the method = "ward.D2".

**Q14**. How well do the hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the `table()` function to compare the output of each model (`wisc.km$cluster` and `wisc.hclust.clusters`) with the vector containing the actual diagnoses.

```{r}
table(wisc.hclust.clusters,diagnosis)
```

The hierarchical clustering models created in previous sections are not as effective as it causes clusters 2 and 4 to not contain enough data to consider the clusters to be significant.

## Prediction

Predicting the diagnosis using the `predict()` function; new cancer data from two patients are downloaded and plotted against the PCA graph:

```{r}
#downloading data
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
```

```{r}
#plotting with the PCA graph
plot(wisc.pr$x[,1:2], col=g)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```

Q16: Which of these new patients should we prioritize for follow up based on your results?

A: We should prioritize patient 2 as it resides within the collection of data that were diagnosed as malicious.
